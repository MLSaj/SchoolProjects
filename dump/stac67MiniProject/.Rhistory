x11reduced <- lm(y ~ x87+ x185+ x62 + x54 + x187 + x96+ x51 + x184, data = r_data)
anova(x11reduced,finalfit)
x87reduced <- lm(y ~ x11+ x185+ x62 + x54 + x187 + x96+ x51 + x184, data = r_data)
anova(x11reduced,finalfit)
CV.K.FOLD <- function(data, form, K = 2){
n <- dim(data)[[1]] ## number of observations ##
Y <- data$Y  ## response variable ##
if(K > n) stop("K should be <= n, number of observations")
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n) ## validation folds ##
ms <- max(s)
MSPE <- 0 ## initiation of the mean squared prediction error ##
coef <- NULL
MSE <- NULL
AY <- PY <- NULL
for(v in 1:ms){
j.in <- c(1:n)[(s != v)]
j.out <- c(1:n)[(s == v)]
data.train <- data[j.in,]
data.test <- data[j.out,]
data.train <- as.data.frame(data.train)
data.test <- as.data.frame(data.test)
fit.train <- lm(form, data = data.train) ## fitting model using training data ##
coef <- cbind(coef, coefficients(fit.train))
MSE <- c(MSE, summary(fit.train)$sigma^2)
pred.test <- predict(fit.train, newdata = data.test) ## prediction for the validated data ##
cat("Validation Set =", v, "\n\n\n") ## validation set number ##
cat("Fitted Model Using Training Data \n\n\n")
print(summary(fit.train)) ## fitted model to the training data ##
print(anova(fit.train)) ## anova table for the fitted model to the training data ##
cat("Observation Numbers =", c(1:n)[j.out], "\n\n\n") ## observation numbers in the validation set ##
cat("Actual Observations =", Y[j.out], "\n\n\n") ## observed response in the validation set ##
cat("Predicted Observations =", pred.test, "\n\n\n") ## predicted response in the validation set ##
MSPE <- MSPE + sum((Y[j.out] - pred.test)^2)
AY <- c(AY, Y[j.out])
PY <- c(PY, pred.test)
}
MSPE <- MSPE/n ## overall mean squared prediction error ##
cat("Overall Mean Squared Prediction Error", MSPE, "\n\n")
coef <- rbind(coef, MSE)
colnames(coef) <- paste("Fold", 1:K, sep = ":")
cat("Estimated Coefficients \n\n")
print(coef)
plot(AY, PY, xlab = "Observed Response", ylab = "Predicted Response", xlim = range(c(AY, PY)), ylim = range(c(AY, PY)), main = "")
}
set.seed(1)
form <- as.formula(y ~ x11 + x87 + x51)
CV.K.FOLD(data = r_data, form, K = 10)
## R function to perform K-fold cross-validation ##
CV.K.FOLD <- function(data, form, K = 10){
n <- dim(data)[[1]] ## number of observations ##
Y <- data$y  ## response variable ##
if(K > n) stop("K should be <= n, number of observations")
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n) ## validation folds ##
ms <- max(s)
MSPE <- 0 ## initiation of the mean squared prediction error ##
coef <- NULL
MSE <- NULL
AY <- PY <- NULL
for(v in 1:ms){
j.in <- c(1:n)[(s != v)]
j.out <- c(1:n)[(s == v)]
data.train <- data[j.in,]
data.test <- data[j.out,]
data.train <- as.data.frame(data.train)
data.test <- as.data.frame(data.test)
fit.train <- lm(form, data = data.train) ## fitting model using training data ##
coef <- cbind(coef, coefficients(fit.train))
MSE <- c(MSE, summary(fit.train)$sigma^2)
pred.test <- predict(fit.train, newdata = data.test) ## prediction for the validated data ##
cat("Validation Set =", v, "\n\n\n") ## validation set number ##
cat("Fitted Model Using Training Data \n\n\n")
print(summary(fit.train)) ## fitted model to the training data ##
print(anova(fit.train)) ## anova table for the fitted model to the training data ##
cat("Observation Numbers =", c(1:n)[j.out], "\n\n\n") ## observation numbers in the validation set ##
cat("Actual Observations =", Y[j.out], "\n\n\n") ## observed response in the validation set ##
cat("Predicted Observations =", pred.test, "\n\n\n") ## predicted response in the validation set ##
MSPE <- MSPE + sum((Y[j.out] - pred.test)^2)
AY <- c(AY, Y[j.out])
PY <- c(PY, pred.test)
}
MSPE <- MSPE/n ## overall mean squared prediction error ##
cat("Overall Mean Squared Prediction Error", MSPE, "\n\n")
coef <- rbind(coef, MSE)
colnames(coef) <- paste("Fold", 1:K, sep = ":")
cat("Estimated Coefficients \n\n")
print(coef)
plot(AY, PY, xlab = "Observed Response", ylab = "Predicted Response", xlim = range(c(AY, PY)), ylim = range(c(AY, PY)), main = "")
}
set.seed(1)
form <- as.formula(y ~ x11 + x87 + x51)
CV.K.FOLD(data = r_data, form, K = 10)
## R function to perform K-fold cross-validation ##
CV.K.FOLD <- function(data, form, K = 10){
n <- dim(data)[[1]] ## number of observations ##
Y <- data$y  ## response variable ##
if(K > n) stop("K should be <= n, number of observations")
f <- ceiling(n/K)
s <- sample(rep(1:K, f), n) ## validation folds ##
ms <- max(s)
MSPE <- 0 ## initiation of the mean squared prediction error ##
coef <- NULL
MSE <- NULL
AY <- PY <- NULL
for(v in 1:ms){
j.in <- c(1:n)[(s != v)]
j.out <- c(1:n)[(s == v)]
data.train <- data[j.in,]
data.test <- data[j.out,]
data.train <- as.data.frame(data.train)
data.test <- as.data.frame(data.test)
fit.train <- lm(form, data = data.train) ## fitting model using training data ##
coef <- cbind(coef, coefficients(fit.train))
MSE <- c(MSE, summary(fit.train)$sigma^2)
pred.test <- predict(fit.train, newdata = data.test) ## prediction for the validated data ##
cat("Validation Set =", v, "\n\n\n") ## validation set number ##
cat("Fitted Model Using Training Data \n\n\n")
print(summary(fit.train)) ## fitted model to the training data ##
print(anova(fit.train)) ## anova table for the fitted model to the training data ##
cat("Observation Numbers =", c(1:n)[j.out], "\n\n\n") ## observation numbers in the validation set ##
cat("Actual Observations =", Y[j.out], "\n\n\n") ## observed response in the validation set ##
cat("Predicted Observations =", pred.test, "\n\n\n") ## predicted response in the validation set ##
MSPE <- MSPE + sum((Y[j.out] - pred.test)^2)
AY <- c(AY, Y[j.out])
PY <- c(PY, pred.test)
}
MSPE <- MSPE/n ## overall mean squared prediction error ##
cat("Overall Mean Squared Prediction Error", MSPE, "\n\n")
coef <- rbind(coef, MSE)
colnames(coef) <- paste("Fold", 1:K, sep = ":")
cat("Estimated Coefficients \n\n")
print(coef)
plot(AY, PY, xlab = "Observed Response", ylab = "Predicted Response", xlim = range(c(AY, PY)), ylim = range(c(AY, PY)), main = "")
}
set.seed(1)
form <- as.formula(y ~ x11 + x87 + x51)
CV.K.FOLD(data = r_data, form, K = 5)
plot(x=r_data$x11, y=r_data$y, xlab = "Predictor Variable x11", ylab ="Response Variable")
plot(x=r_data$x87, y=r_data$y, xlab = "Predictor Variable x87", ylab ="Response Variable")
plot(x=r_data$x51, y=r_data$y, xlab = "Predictor Variable x51", ylab ="Response Variable")
fit <- lm(y~x11 + x87 + x51, data = r_data)
fit
fitted(fit)
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
head(finaldata)
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predictedvalues
X <-as.matrix(cbin(1,finaldata[]))
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predictedvalues
X <-as.matrix(cbind(1,finaldata))
X
X <-as.matrix(cbind(1,finaldata))
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hat_mat
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
hii
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predicted values
X <-as.matrix(cbind(1,finaldata))
#Influential y observations
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
MSE <- (summary(fit)$sigma)^2 ## mean squared error
s2ei <- MSE * (1 - hii) # estimated variance of the ith residual
I <- matrix(0, nrow = dim(hat_mat)[[1]], ncol = dim(hat_mat)[[1]])
diag(I) <- 1 ## I: identity matrix
s2e <- MSE * (I - hat_mat) # estimated variance-covariance matrix of the residuals
ri <- ei/sqrt(s2ei) #studentized residuals
n <- dim(finaldata)[[1]]
p <- 4 ## number of regression coefficients parameters
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predicted values
ei <- residuals(fit) # residuals
X <-as.matrix(cbind(1,finaldata))
#Influential y observations
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
MSE <- (summary(fit)$sigma)^2 ## mean squared error
s2ei <- MSE * (1 - hii) # estimated variance of the ith residual
I <- matrix(0, nrow = dim(hat_mat)[[1]], ncol = dim(hat_mat)[[1]])
diag(I) <- 1 ## I: identity matrix
s2e <- MSE * (I - hat_mat) # estimated variance-covariance matrix of the residuals
ri <- ei/sqrt(s2ei) #studentized residuals
n <- dim(finaldata)[[1]]
p <- 4 ## number of regression coefficients parameters
SSE <- sum(ei^2) ##error sum of squares
ti <- ei * sqrt((n - p - 1)/(SSE * (1 - hii) - ei^2)) # studentized deleted residuals
alpha <- 0.05
BCV <- qt(1 - alpha/(2*n), n - p - 1) ## Bonferroni Critical Value
absti <- abs(ti)
index_of_outlier <- c(1:n)[absti >= BCV]
index_of_outlier
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predicted values
ei <- residuals(fit) # residuals
X <-as.matrix(cbind(1,finaldata))
#Influential y observations
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
MSE <- (summary(fit)$sigma)^2 ## mean squared error
s2ei <- MSE * (1 - hii) # estimated variance of the ith residual
I <- matrix(0, nrow = dim(hat_mat)[[1]], ncol = dim(hat_mat)[[1]])
diag(I) <- 1 ## I: identity matrix
s2e <- MSE * (I - hat_mat) # estimated variance-covariance matrix of the residuals
ri <- ei/sqrt(s2ei) #studentized residuals
n <- dim(finaldata)[[1]]
p <- 4 ## number of regression coefficients parameters
SSE <- sum(ei^2) ##error sum of squares
ti <- ei * sqrt((n - p - 1)/(SSE * (1 - hii) - ei^2)) # studentized deleted residuals
alpha <- 0.05
BCV <- qt(1 - alpha/(2*n), n - p - 1) ## Bonferroni Critical Value
absti <- abs(ti)
index_of_outlier <- c(1:n)[absti >= BCV]
hii <- diag(hat_mat) ## diagonal elements of that hat matrix
hsum <- sum(hii)
hmean <- hsum/n
cutoff <- 2 * hmean
index_of_outlier <- c(1:n)[hii > cutoff]
index_of_outlier
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predicted values
ei <- residuals(fit) # residuals
X <-as.matrix(cbind(1,finaldata))
#Influential y observations
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
MSE <- (summary(fit)$sigma)^2 ## mean squared error
s2ei <- MSE * (1 - hii) # estimated variance of the ith residual
I <- matrix(0, nrow = dim(hat_mat)[[1]], ncol = dim(hat_mat)[[1]])
diag(I) <- 1 ## I: identity matrix
s2e <- MSE * (I - hat_mat) # estimated variance-covariance matrix of the residuals
ri <- ei/sqrt(s2ei) #studentized residuals
n <- dim(finaldata)[[1]]
p <- 4 ## number of regression coefficients parameters
SSE <- sum(ei^2) ##error sum of squares
ti <- ei * sqrt((n - p - 1)/(SSE * (1 - hii) - ei^2)) # studentized deleted residuals
alpha <- 0.05
BCV <- qt(1 - alpha/(2*n), n - p - 1) ## Bonferroni Critical Value
absti <- abs(ti)
index_of_outlier <- c(1:n)[absti >= BCV]
hii <- diag(hat_mat) ## diagonal elements of that hat matrix
hsum <- sum(hii)
hmean <- hsum/n
cutoff <- 2 * hmean
index_of_outlier <- c(1:n)[hii > cutoff]
dffits <- ti * sqrt(hii/(1 - hii))
adffits <- abs(dffits)
cutoff <- 1
index_of_infl <- c(1:n)[adffits > cutoff]
index_of_infl
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predicted values
ei <- residuals(fit) # residuals
X <-as.matrix(cbind(1,finaldata))
#Influential y observations
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
MSE <- (summary(fit)$sigma)^2 ## mean squared error
s2ei <- MSE * (1 - hii) # estimated variance of the ith residual
I <- matrix(0, nrow = dim(hat_mat)[[1]], ncol = dim(hat_mat)[[1]])
diag(I) <- 1 ## I: identity matrix
s2e <- MSE * (I - hat_mat) # estimated variance-covariance matrix of the residuals
ri <- ei/sqrt(s2ei) #studentized residuals
n <- dim(finaldata)[[1]]
p <- 4 ## number of regression coefficients parameters
SSE <- sum(ei^2) ##error sum of squares
ti <- ei * sqrt((n - p - 1)/(SSE * (1 - hii) - ei^2)) # studentized deleted residuals
alpha <- 0.05
BCV <- qt(1 - alpha/(2*n), n - p - 1) ## Bonferroni Critical Value
absti <- abs(ti)
index_of_outlier <- c(1:n)[absti >= BCV]
hii <- diag(hat_mat) ## diagonal elements of that hat matrix
hsum <- sum(hii)
hmean <- hsum/n
cutoff <- 2 * hmean
index_of_outlier <- c(1:n)[hii > cutoff]
dffits <- ti * sqrt(hii/(1 - hii))
adffits <- abs(dffits)
cutoff <- 1
index_of_infl <- c(1:n)[adffits > cutoff]
Di <- ((ei^2)/ (p * MSE)) * (hii/(1 - hii)^2)
PERF <- pf(Di, df1 = p, df2 = (n - p))
cbind(Di, PERF)
index_of_infl <- c(1:n)[PERF >= 0.50]
index_of_infl
#VIF
VIF(finaldata)
cor(finaldata)
finalcolnames = c("x11","x87","x51");
finaldata <- r_data[ , (names(r_data) %in% finalcolnames)]
fit <- lm(y~x11 + x87 + x51, data = r_data)
yhati <- fitted(fit) #predicted values
ei <- residuals(fit) # residuals
X <-as.matrix(cbind(1,finaldata))
#Influential y observations
hat_mat <- X %*% solve(t(X) %*% X) %*% t(X) ## hat matrix
hii <- diag(hat_mat)
MSE <- (summary(fit)$sigma)^2 ## mean squared error
s2ei <- MSE * (1 - hii) # estimated variance of the ith residual
I <- matrix(0, nrow = dim(hat_mat)[[1]], ncol = dim(hat_mat)[[1]])
diag(I) <- 1 ## I: identity matrix
s2e <- MSE * (I - hat_mat) # estimated variance-covariance matrix of the residuals
ri <- ei/sqrt(s2ei) #studentized residuals
n <- dim(finaldata)[[1]]
p <- 4 ## number of regression coefficients parameters
SSE <- sum(ei^2) ##error sum of squares
ti <- ei * sqrt((n - p - 1)/(SSE * (1 - hii) - ei^2)) # studentized deleted residuals
alpha <- 0.05
BCV <- qt(1 - alpha/(2*n), n - p - 1) ## Bonferroni Critical Value
absti <- abs(ti)
index_of_outlier <- c(1:n)[absti >= BCV]
hii <- diag(hat_mat) ## diagonal elements of that hat matrix
hsum <- sum(hii)
hmean <- hsum/n
cutoff <- 2 * hmean
index_of_outlier <- c(1:n)[hii > cutoff]
dffits <- ti * sqrt(hii/(1 - hii))
adffits <- abs(dffits)
cutoff <- 2/sqrt(p/n) #large dataset
index_of_infl <- c(1:n)[adffits > cutoff]
index_of_infl
dffits <- ti * sqrt(hii/(1 - hii))
adffits <- abs(dffits)
cutoff <- 1 #200 is a medium data set
index_of_infl <- c(1:n)[adffits > cutoff]
index_of_infl
dffits <- ti * sqrt(hii/(1 - hii))
adffits <- abs(dffits)
cutoff <- 2*sqrt(p/n) #200 is a medium data set
index_of_infl <- c(1:n)[adffits > cutoff]
index_of_infl
p
n
p/n
sqrt(p/n)
cutoff <- 2*sqrt(p/n) #200 is a large dataset
index_of_infl <- c(1:n)[adffits > cutoff]
index_of_infl
n
DFBETAS <- function(data, form){
## we assume that the response variable is in the first column of data ##
n <- dim(data)[[1]] ## number of observations ##
X <- as.matrix(cbind(1, data[, - 1])) ## X matrix
invXX <- solve(t(X) %*% X) ## inverse of t(X)X ##
dfbetas <- NULL
for(i in 1:n){
j.in <- c(1:n)[(c(1:n) != i)]
data.train <- data[j.in,]
data.train <- as.data.frame(data.train)
fit.all <- lm(form, data = data) ## fitting model using all of the observations
coef.all <- coefficients(fit.all) # coefficients of the model that uses all of the observations
fit.train <- lm(form, data = data.train) ## fitting model using training data ##
coef.train <- coefficients(fit.train) # coefficients of the model that uses all of the observations
MSE_i <- summary(fit.train)$sigma^2
c_kk <- diag(invXX)
dfbetas_i <- (coef.all - coef.train)/sqrt(MSE_i * c_kk)
dfbetas <- rbind(dfbetas, dfbetas_i)
}
rownames(dfbetas) <- c(1:n)
print(dfbetas)
}
finalcolnamesY = c("y", "x11","x87","x51");
finaldataY <- r_data[ , (names(r_data) %in% finalcolnamesY)]
form <- as.formula(y ~ x11 +x87 + x51)
DFBETAS(data = finaldataY, form)
VIF <- function(data){
## first column of "data" contains the response variable ##
## the columns starting from second to the last contain the predictor variables ##
p <- dim(data)[[2]]
colnames(data) <- c("Y", paste("X", c(1:(p - 1)), sep = ""))
vif <- NULL
for(i in 1:(p - 1)){
form <- paste(paste("X", i, sep = ""), paste(paste("X", c(1:(p - 1))[-i], sep = ""), collapse=" + "), sep = " ~ ")
form <- as.formula(form)
fit.reg <- lm(form, data = data)
R2 <- summary(fit.reg)$r.squared
vifk <- 1/(1 - R2)
vif <- c(vif, vifk)
}
cat("Variance Inflation Factors = ", vif, "\n\n")
maxvif <- max(vif)
cat("Maximum VIF = ", maxvif, "\n\n")
meanvif <- mean(vif)
cat("Mean VIF = ", meanvif, "\n\n")
}
VIF(finaldataY)
pairs(finaldataY)
cor(finaldataY)
fit <- lm(y~x11 + x87 + x51, data = r_data)
fit
dffits <- ti * sqrt(hii/(1 - hii))
adffits <- abs(dffits)
cutoff <- 2*sqrt(p/n) #200 is a large dataset
index_of_infl <- c(1:n)[adffits > cutoff]
index_of_infl
sr.walk <- function(i){
j <- sample(c(i - 1, i + 1), 1, replace = TRUE)
return(j)
}
MH_Algo_D <- function(N = 1000, start.srw = 0){
## N: Total number of values in the chain ##
i <- start.srw
x <- rep(NA, (N+1))
x[1] <- i
for(n in c(1:N)){
j <- sr.walk(i)  ## proposing j ##
aij <- min(c(1, (((j - (1/2))^4) * (exp(-3*abs(j)))*((cos(j))^2))/(((i - (1/2))^4) * (exp(-3*abs(i)))*((cos(i))^2)))) ## Acceptance Probability ##
rn <- runif(1, min = 0, max = 1) ## Uniform Random Number ##
ifelse(rn <= aij, x[n+1] <- j, x[n+1] <- i) ## Accepting/Rejecting the proposal ##
i <- x[n+1]
}
return(x)
}
set.seed(7)
x <- MH_Algo_D(N = 110000, start.srw = -20)
xx
x
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
sr.walk <- function(i){
j <- sample(c(i - 1, i + 1), 1, replace = TRUE)
return(j)
}
MH_Algo_D <- function(N = 1000, start.srw = 0){
## N: Total number of values in the chain ##
i <- start.srw
x <- rep(NA, (N+1))
x[1] <- i
for(n in c(1:N)){
j <- sr.walk(i)  ## proposing j ##
aij <- min(c(1, (((j - (1/2))^4) * (exp(-3*abs(j)))*((cos(j))^2))/(((i - (1/2))^4) * (exp(-3*abs(i)))*((cos(i))^2)))) ## Acceptance Probability ##
rn <- runif(1, min = 0, max = 1) ## Uniform Random Number ##
ifelse(rn <= aij, x[n+1] <- j, x[n+1] <- i) ## Accepting/Rejecting the proposal ##
i <- x[n+1]
}
return(x)
}
set.seed(7)
x <- MH_Algo_D(N = 110000, start.srw = -20)
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
abline(v = 10000)
#After Burn-in
B <- 10000 # Burn-in
x <- x[-c(1:B)]
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
sr.walk <- function(i){
j <- sample(c(i - 1, i + 1), 1, replace = TRUE)
return(j)
}
MH_Algo_D <- function(N = 1000, start.srw = 0){
## N: Total number of values in the chain ##
i <- start.srw
x <- rep(NA, (N+1))
x[1] <- i
for(n in c(1:N)){
j <- sr.walk(i)  ## proposing j ##
aij <- min(c(1, (((j - (1/2))^4) * (exp(-3*abs(j)))*((cos(j))^2))/(((i - (1/2))^4) * (exp(-3*abs(i)))*((cos(i))^2)))) ## Acceptance Probability ##
rn <- runif(1, min = 0, max = 1) ## Uniform Random Number ##
ifelse(rn <= aij, x[n+1] <- j, x[n+1] <- i) ## Accepting/Rejecting the proposal ##
i <- x[n+1]
}
return(x)
}
set.seed(7)
x <- MH_Algo_D(N = 110000, start.srw = -20)
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
abline(v = 10000)
#After Burn-in
B <- 10000 # Burn-in
x <- x[-c(1:B)]
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
MH_Algo_C <- function(N = 1000, start.chain = 0){
## N: Total number of values in the chain ##
x <- start.chain
X <- rep(NA, (N+1))
X[1] <- x
for(n in c(1:N)){
y <- rnorm(1, mean = x, sd = 1)  ## proposing y ##
axy <- min(c(1, (((1+abs(y))/(1+abs(x)))^3)*exp(-(y^4-x^4)))) ## Acceptance Probability ##
rn <- runif(1, min = 0, max = 1) ## Uniform Random Number ##
ifelse(rn <= axy, X[n+1] <- y, X[n+1] <- x) ## Accepting/Rejecting the proposal ##
x <- X[n+1]
}
return(X)
}
set.seed(7)
x <- MH_Algo_C(N = 110000, start.chain = -50)
# Plot before burn-in
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
abline(v = 10000)
# Plot before burn-in
B <- 10000
x <- x[-c(1:B)]
plot(c(1:length(x)), x, ylim = range(x), xlab = "Time", ylab = "Values of the Chain")
acf(x)
length(x)
